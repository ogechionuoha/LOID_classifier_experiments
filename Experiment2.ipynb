{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup:\n",
    "\n",
    "#### Model: Resnet 50, Pretrained, \n",
    "#### Action: Finetune\n",
    "#### Visualisation:  GradCAM\n",
    "#### Dataset: Full, unbalanced\n",
    "#### Uses Weighted Loss: True\n",
    "#### Uses Oversampling: False\n",
    "#### Attention Module: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K1TwBuB0AXM-"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from helper_fns import *\n",
    "from resnet_models import ResNet50_GradCam\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5Keyq9w-AgrY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['not_uk', 'uk'], 'loading complete in 0m 6s')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data preprocessing\n",
    "data = '/workspace/loid/images/old_uk_not_uk/train/'\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "tr_transform = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "#test_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "tr_dataset = datasets.ImageFolder(data, transform=tr_transform)\n",
    "\n",
    "end = time.time() - begin\n",
    "class_names = tr_dataset.classes\n",
    "class_names, f'loading complete in {end // 60:.0f}m {end % 60:.0f}s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Z24_jZmqBOeX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(978827, 30701)\n"
     ]
    }
   ],
   "source": [
    "tr_class_counts = np.sum(np.array(tr_dataset.targets) == 0), np.sum(np.array(tr_dataset.targets) == 1)\n",
    "\n",
    "print(tr_class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A2WRgzFOBVuO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(757146, 151429, 100953)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_count, val_count = int(0.75 * len(tr_dataset)), int(0.15 * len(tr_dataset))\n",
    "test_count = len(tr_dataset) - (train_count + val_count)\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(tr_dataset, [train_count, val_count, test_count])\n",
    "dataset_sizes = len(train_set), len(val_set), len(test_set)\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_nH3Z2eGBe0X"
   },
   "outputs": [],
   "source": [
    "#loaders\n",
    "batchsize = 32\n",
    "workers = 1\n",
    "pinmemory = True\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batchsize, num_workers=workers, shuffle=True, pin_memory=pinmemory)\n",
    "valid_loader = torch.utils.data.DataLoader(val_set, batch_size=batchsize, num_workers=workers, shuffle=True, pin_memory=pinmemory)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batchsize, num_workers=workers, shuffle=True, pin_memory=pinmemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M0A-E7IcB-k1"
   },
   "outputs": [],
   "source": [
    "def train(model, criterion_set, optimizer, epochs, scheduler, class_names, device = 'cpu', board_writer=None, save_folder = None):\n",
    "    model = model.to(device)\n",
    "    valid_loss_min = np.Inf\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f'Epoch {epoch}/{epochs-1}')\n",
    "        print('-'*15)\n",
    "\n",
    "        for phase in ['train','val']:\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "\n",
    "            class_probs = []\n",
    "            class_preds = []\n",
    "            y_target = np.array([])\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                loader = train_loader\n",
    "                dataset_size = dataset_sizes[0]\n",
    "                filename = save_folder+'/train.csv'\n",
    "                criterion = criterion_set[0]\n",
    "            else:\n",
    "                model.eval()\n",
    "                loader = valid_loader\n",
    "                dataset_size = dataset_sizes[1]\n",
    "                filename = save_folder+'/val.csv'\n",
    "                criterion = criterion_set[1]\n",
    "            i = 0\n",
    "            begin_phase = time.time()\n",
    "            for inputs, labels in loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                y_target = np.concatenate((y_target,labels.cpu()))\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    class_probs_batch = [F.softmax(output, dim=0) for output in outputs]\n",
    "                    class_probs.append(class_probs_batch)\n",
    "                    class_preds.append(preds.cpu())\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_correct += torch.sum(preds == labels.data)\n",
    "                    \n",
    "                end = time.time() - begin_phase\n",
    "                i+=1\n",
    "                if i%1000 == 0:\n",
    "                    print(f'Phase: {phase} Batch {i} complete in {end // 60:.0f}m {end % 60:.0f}s')\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "            epoch_preds = torch.cat(class_preds)\n",
    "\n",
    "            for i in range(len(class_names)):\n",
    "                add_pr_curve_tensorboard(board_writer, phase, i, epoch_probs, epoch_preds, class_names, global_step=epoch)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            epoch_accuracy = running_correct.double() / dataset_size\n",
    "            \n",
    "            # save model if validation loss has decreased\n",
    "            if phase == 'val' and epoch_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saved updated model.'.format(valid_loss_min, epoch_loss))\n",
    "                torch.save(model.state_dict(), save_folder+'/checkpoint.pt')\n",
    "                valid_loss_min = epoch_loss\n",
    "\n",
    "            log_metrics(filename, y_target, epoch_preds, epoch_loss, epoch_loss <= valid_loss_min, writer=board_writer, epoch=epoch)\n",
    "\n",
    "            time_elapsed = time.time() - start\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}; Accuracy: {epoch_accuracy:.4f}; Completed in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "\n",
    "            if board_writer is not None:\n",
    "                board_writer.add_scalar(f'{phase}/loss', epoch_loss, epoch)\n",
    "                board_writer.add_scalar(f'{phase}/accuracy', epoch_accuracy, epoch)\n",
    "\n",
    "        print()\n",
    "\n",
    "    if board_writer is not None:\n",
    "        board_writer.flush()\n",
    "        board_writer.close()\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wIFkeZSaCC-Q"
   },
   "outputs": [],
   "source": [
    "#model settings\n",
    "model_name = 'resnet50_finetuned_gradcam'\n",
    "results_folder = './results/'\n",
    "save_folder = results_folder + model_name\n",
    "train_results = save_folder +'/train.csv'\n",
    "val_results = save_folder +'/val.csv'\n",
    "checkpoint_path = save_folder +'/checkpoint.pt'\n",
    "\n",
    "logs = SummaryWriter(f'./logs/{model_name}')\n",
    "\n",
    "if not os.path.isdir(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "    \n",
    "if not os.path.isdir(results_folder + model_name):\n",
    "    os.mkdir(results_folder + model_name)\n",
    "      \n",
    "with open(train_results, 'a') as train_result:\n",
    "    header = \",\".join(['loss', 'accuracy', 'tn', 'fp', 'fn', 'tp', 'precision_0', 'precision_1', 'recall_0', 'recall_1', 'f1_0', 'f1_1', 'count_0', 'count_1','auroc','\\n'])\n",
    "    train_result.write(header)\n",
    "\n",
    "with open(val_results, 'a') as val_result:\n",
    "    header = \",\".join(['loss', 'accuracy', 'tn', 'fp', 'fn', 'tp', 'precision_0', 'precision_1', 'recall_0', 'recall_1', 'f1_0', 'f1_1', 'count_0', 'count_1','auroc','saved','\\n'])\n",
    "    val_result.write(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "A5URiOuMCJdL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loading complete in 0m 3s'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualise batch\n",
    "\n",
    "# Make a grid from batch\n",
    "begin = time.time()\n",
    "inputs, classes = next(iter(valid_loader))\n",
    "end = time.time() - begin\n",
    "img_out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "#imshow(img_out, title=[class_names[x] for x in classes])\n",
    "\n",
    "#add to tensorboard\n",
    "#logs.add_image('Sample input image',img_out)\n",
    "\n",
    "f'loading complete in {end // 60:.0f}m {end % 60:.0f}s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cvW6YJnyCNWm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.3623e-06, 4.3350e-05]),\n",
       " tensor([6.8082e-06, 2.1988e-04]),\n",
       " tensor([1.0218e-05, 3.2415e-04]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate loss weights \n",
    "tr_loss_weights = 1.0/torch.Tensor([np.sum(np.array(train_set.dataset.targets)[train_set.indices] == 0) , np.sum(np.array(train_set.dataset.targets)[train_set.indices] == 1)])\n",
    "valid_loss_weights = 1.0/torch.Tensor([np.sum(np.array(val_set.dataset.targets)[val_set.indices] == 0) , np.sum(np.array(val_set.dataset.targets)[val_set.indices] == 1)])\n",
    "test_loss_weights = 1.0/torch.Tensor([np.sum(np.array(test_set.dataset.targets)[test_set.indices] == 0) , np.sum(np.array(test_set.dataset.targets)[test_set.indices] == 1)])\n",
    "tr_loss_weights, valid_loss_weights, test_loss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6yE-QdJyCOKp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scaled loss function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Get a batch of data\\ntime1 = time.time()\\nimages, classes = next(iter(valid_loader))\\nmodel.to(device)\\nimages = images.to(device)\\nout = model(images)\\ntime2 = time.time() - time1\\n\\n\\nlogs.add_graph(model, images)\\nlogs.close()\\n\\nprint(f'{time2 // 60:.0f}m {time2 % 60:.0f}s')\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= ResNet50_GradCam(num_classes=len(class_names), visualise=False, pretrained=True, finetune=True)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "#set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "balance_loss = True\n",
    "\n",
    "if balance_loss:\n",
    "    print(\"Using scaled loss function\")\n",
    "    train_criterion = nn.CrossEntropyLoss(weight=tr_loss_weights.to(device))\n",
    "    valid_criterion = nn.CrossEntropyLoss(weight=valid_loss_weights.to(device))\n",
    "else:\n",
    "    print(\"Using unscaled loss function\")\n",
    "    train_criterion = nn.CrossEntropyLoss()\n",
    "    valid_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "criterion_set = [train_criterion, valid_criterion]\n",
    "\n",
    "'''\n",
    "# Get a batch of data\n",
    "time1 = time.time()\n",
    "images, classes = next(iter(valid_loader))\n",
    "model.to(device)\n",
    "images = images.to(device)\n",
    "out = model(images)\n",
    "time2 = time.time() - time1\n",
    "\n",
    "\n",
    "logs.add_graph(model, images)\n",
    "logs.close()\n",
    "\n",
    "print(f'{time2 // 60:.0f}m {time2 % 60:.0f}s')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tMINj42DCT3z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Etj5hZ1-CWkK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/89\n",
      "---------------\n",
      "Phase: train Batch 1000 complete in 5m 6s\n",
      "Phase: train Batch 2000 complete in 10m 19s\n",
      "Phase: train Batch 3000 complete in 15m 34s\n",
      "Phase: train Batch 4000 complete in 20m 49s\n",
      "Phase: train Batch 5000 complete in 26m 4s\n",
      "Phase: train Batch 6000 complete in 31m 20s\n",
      "Phase: train Batch 7000 complete in 36m 36s\n",
      "Phase: train Batch 8000 complete in 41m 52s\n",
      "Phase: train Batch 9000 complete in 47m 9s\n",
      "Phase: train Batch 10000 complete in 52m 25s\n",
      "Phase: train Batch 11000 complete in 57m 42s\n",
      "Phase: train Batch 12000 complete in 62m 59s\n",
      "Phase: train Batch 13000 complete in 68m 15s\n",
      "Phase: train Batch 14000 complete in 73m 32s\n",
      "Phase: train Batch 15000 complete in 78m 49s\n",
      "Phase: train Batch 16000 complete in 84m 6s\n",
      "Phase: train Batch 17000 complete in 89m 23s\n",
      "Phase: train Batch 18000 complete in 94m 40s\n",
      "Phase: train Batch 19000 complete in 99m 57s\n",
      "Phase: train Batch 20000 complete in 105m 15s\n",
      "Phase: train Batch 21000 complete in 110m 32s\n",
      "Phase: train Batch 22000 complete in 115m 50s\n",
      "Phase: train Batch 23000 complete in 121m 7s\n",
      "train Loss: 0.5999; Accuracy: 0.8336; Completed in 124m 42s\n",
      "Phase: val Batch 1000 complete in 2m 58s\n",
      "Phase: val Batch 2000 complete in 5m 56s\n",
      "Phase: val Batch 3000 complete in 8m 54s\n",
      "Phase: val Batch 4000 complete in 11m 51s\n",
      "Validation loss decreased (inf --> 0.541340).  Saved updated model.\n",
      "val Loss: 0.5413; Accuracy: 0.8283; Completed in 138m 49s\n",
      "\n",
      "Epoch 1/89\n",
      "---------------\n",
      "Phase: train Batch 1000 complete in 5m 18s\n",
      "Phase: train Batch 2000 complete in 10m 36s\n",
      "Phase: train Batch 3000 complete in 15m 53s\n",
      "Phase: train Batch 4000 complete in 21m 10s\n",
      "Phase: train Batch 5000 complete in 26m 27s\n",
      "Phase: train Batch 6000 complete in 31m 45s\n",
      "Phase: train Batch 7000 complete in 37m 2s\n",
      "Phase: train Batch 8000 complete in 42m 19s\n",
      "Phase: train Batch 9000 complete in 47m 37s\n",
      "Phase: train Batch 10000 complete in 52m 54s\n",
      "Phase: train Batch 11000 complete in 58m 12s\n",
      "Phase: train Batch 12000 complete in 63m 30s\n",
      "Phase: train Batch 13000 complete in 68m 47s\n",
      "Phase: train Batch 14000 complete in 74m 5s\n",
      "Phase: train Batch 15000 complete in 79m 23s\n",
      "Phase: train Batch 16000 complete in 84m 41s\n",
      "Phase: train Batch 17000 complete in 89m 59s\n",
      "Phase: train Batch 18000 complete in 95m 16s\n",
      "Phase: train Batch 19000 complete in 100m 34s\n",
      "Phase: train Batch 20000 complete in 105m 52s\n",
      "Phase: train Batch 21000 complete in 111m 10s\n",
      "Phase: train Batch 22000 complete in 116m 28s\n",
      "Phase: train Batch 23000 complete in 121m 46s\n",
      "train Loss: 0.5228; Accuracy: 0.8405; Completed in 264m 9s\n",
      "Phase: val Batch 1000 complete in 2m 58s\n",
      "Phase: val Batch 2000 complete in 5m 54s\n",
      "Phase: val Batch 3000 complete in 8m 51s\n",
      "Phase: val Batch 4000 complete in 11m 48s\n",
      "Validation loss decreased (0.541340 --> 0.501510).  Saved updated model.\n",
      "val Loss: 0.5015; Accuracy: 0.8404; Completed in 278m 11s\n",
      "\n",
      "Epoch 2/89\n",
      "---------------\n",
      "Phase: train Batch 1000 complete in 5m 18s\n",
      "Phase: train Batch 2000 complete in 10m 36s\n",
      "Phase: train Batch 3000 complete in 15m 53s\n",
      "Phase: train Batch 4000 complete in 21m 11s\n",
      "Phase: train Batch 5000 complete in 26m 28s\n",
      "Phase: train Batch 6000 complete in 31m 46s\n",
      "Phase: train Batch 7000 complete in 37m 4s\n",
      "Phase: train Batch 8000 complete in 42m 21s\n",
      "Phase: train Batch 9000 complete in 47m 39s\n",
      "Phase: train Batch 10000 complete in 52m 57s\n",
      "Phase: train Batch 11000 complete in 58m 15s\n",
      "Phase: train Batch 12000 complete in 63m 33s\n",
      "Phase: train Batch 13000 complete in 68m 51s\n",
      "Phase: train Batch 14000 complete in 74m 9s\n",
      "Phase: train Batch 15000 complete in 79m 27s\n",
      "Phase: train Batch 16000 complete in 84m 45s\n",
      "Phase: train Batch 17000 complete in 90m 3s\n",
      "Phase: train Batch 18000 complete in 95m 21s\n",
      "Phase: train Batch 19000 complete in 100m 39s\n",
      "Phase: train Batch 20000 complete in 105m 57s\n",
      "Phase: train Batch 21000 complete in 111m 15s\n",
      "Phase: train Batch 22000 complete in 116m 33s\n",
      "Phase: train Batch 23000 complete in 121m 52s\n",
      "train Loss: 0.4948; Accuracy: 0.8457; Completed in 403m 37s\n",
      "Phase: val Batch 1000 complete in 2m 59s\n",
      "Phase: val Batch 2000 complete in 5m 56s\n",
      "Phase: val Batch 3000 complete in 8m 54s\n",
      "Phase: val Batch 4000 complete in 11m 51s\n",
      "Validation loss decreased (0.501510 --> 0.480804).  Saved updated model.\n",
      "val Loss: 0.4808; Accuracy: 0.8283; Completed in 417m 43s\n",
      "\n",
      "Epoch 3/89\n",
      "---------------\n",
      "Phase: train Batch 1000 complete in 5m 18s\n",
      "Phase: train Batch 2000 complete in 10m 36s\n",
      "Phase: train Batch 3000 complete in 15m 53s\n",
      "Phase: train Batch 4000 complete in 21m 11s\n",
      "Phase: train Batch 5000 complete in 26m 29s\n",
      "Phase: train Batch 6000 complete in 31m 47s\n",
      "Phase: train Batch 7000 complete in 37m 4s\n",
      "Phase: train Batch 8000 complete in 42m 22s\n",
      "Phase: train Batch 9000 complete in 47m 40s\n",
      "Phase: train Batch 10000 complete in 52m 58s\n",
      "Phase: train Batch 11000 complete in 58m 16s\n",
      "Phase: train Batch 13000 complete in 68m 52s\n",
      "Phase: train Batch 14000 complete in 74m 10s\n",
      "Phase: train Batch 15000 complete in 79m 28s\n",
      "Phase: train Batch 16000 complete in 84m 46s\n",
      "Phase: train Batch 17000 complete in 90m 4s\n",
      "Phase: train Batch 18000 complete in 95m 23s\n",
      "Phase: train Batch 19000 complete in 100m 41s\n",
      "Phase: train Batch 20000 complete in 105m 59s\n",
      "Phase: train Batch 21000 complete in 111m 17s\n",
      "Phase: train Batch 22000 complete in 116m 36s\n",
      "Phase: train Batch 23000 complete in 121m 54s\n",
      "train Loss: 0.4769; Accuracy: 0.8496; Completed in 543m 10s\n",
      "Phase: val Batch 1000 complete in 2m 59s\n",
      "Phase: val Batch 2000 complete in 5m 57s\n",
      "Phase: val Batch 3000 complete in 8m 55s\n",
      "Phase: val Batch 4000 complete in 11m 53s\n",
      "Validation loss decreased (0.480804 --> 0.475838).  Saved updated model.\n",
      "val Loss: 0.4758; Accuracy: 0.8650; Completed in 557m 20s\n",
      "\n",
      "Epoch 4/89\n",
      "---------------\n",
      "Phase: train Batch 1000 complete in 5m 18s\n"
     ]
    }
   ],
   "source": [
    "train(model, criterion_set, optimizer, epochs= 90, scheduler=exp_lr_scheduler,class_names= class_names, device = device, board_writer=logs, save_folder = save_folder)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Vanillaresnet50Unbalanced.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
