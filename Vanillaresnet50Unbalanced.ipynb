{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanillaresnet50Unbalanced.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1TwBuB0AXM-"
      },
      "source": [
        "#imports\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.tensorboard import SummaryWriter \n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc, precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
        "\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Keyq9w-AgrY"
      },
      "source": [
        "# data preprocessing\n",
        "data = '/nfs/images/uk_not_uk/train/'\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "begin = time.time()\n",
        "\n",
        "tr_transform = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "#test_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
        "\n",
        "tr_dataset = datasets.ImageFolder(data, transform=tr_transform)\n",
        "#test_dataset = datasets.ImageFolder('./hymenoptera_data/val', transform=test_transform)\n",
        "\n",
        "end = time.time() - begin\n",
        "class_names = tr_dataset.classes\n",
        "class_names, f'loading complete in {end // 60:.0f}m {end % 60:.0f}s'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z24_jZmqBOeX"
      },
      "source": [
        "tr_class_counts = np.sum(np.array(tr_dataset.targets) == 0), np.sum(np.array(tr_dataset.targets) == 1)\n",
        "#test_class_counts = np.sum(np.array(test_dataset.targets) == 0), np.sum(np.array(test_dataset.targets) == 1)\n",
        "\n",
        "print(tr_class_counts)\n",
        "#, test_class_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2WRgzFOBVuO"
      },
      "source": [
        "train_count, val_count = int(0.75 * len(tr_dataset)), int(0.15 * len(tr_dataset))\n",
        "test_count = len(tr_dataset) - (train_count + val_count)\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(tr_dataset, [train_count, val_count, test_count])\n",
        "dataset_sizes = len(train_set), len(val_set), len(test_set)\n",
        "dataset_sizes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nH3Z2eGBe0X"
      },
      "source": [
        "#loaders\n",
        "batchsize = 64\n",
        "workers = 1\n",
        "pinmemory = True\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batchsize, num_workers=workers, shuffle=True, pin_memory=pinmemory)\n",
        "valid_loader = torch.utils.data.DataLoader(val_set, batch_size=batchsize, num_workers=workers, shuffle=True, pin_memory=pinmemory)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batchsize, num_workers=workers, shuffle=True, pin_memory=pinmemory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F2tmmsVBj9E"
      },
      "source": [
        "# helper functions\n",
        "def add_pr_curve_tensorboard(writer, phase, targets, probs, preds, global_step=0):\n",
        "    '''\n",
        "    Takes in a target-class and plots the corresponding precision-recall curve\n",
        "    '''\n",
        "    tensorboard_preds = preds == targets\n",
        "    tensorboard_probs = probs[:, targets]\n",
        "\n",
        "    writer.add_pr_curve(f'{phase}/pr_curve/{class_names[targets]}', tensorboard_preds, tensorboard_probs, global_step=global_step)\n",
        "    writer.flush()\n",
        "    writer.close()\n",
        "    \n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    \n",
        "def log_metrics(filename, y_target, y_pred, loss, saved=False):\n",
        "    \n",
        "    with open(filename, 'a') as result:\n",
        "        a, b = confusion_matrix(y_target, y_pred)\n",
        "        tn, fp, fn, tp = *a, *b\n",
        "        precision, recall, f1, support = precision_recall_fscore_support(y_target, y_pred)\n",
        "        precision_0, precision_1, recall_0, recall_1, f1_0, f1_1, count_0, count_1 = *precision, *recall, *f1, *support\n",
        "        accuracy = accuracy_score(y_target, y_pred)\n",
        "        auroc = roc_auc_score(y_target, y_pred)\n",
        "        line = \",\".join([str(loss), str(accuracy), str(tn), str(fp), str(fn), str(tp), str(precision_0), str(precision_1), str(recall_0), str(recall_1), str(f1_0), str(f1_1), str(count_0), str(count_1), str(auroc), str(saved)+'\\n'])\n",
        "        result.write(line)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0A-E7IcB-k1"
      },
      "source": [
        "def train(model, criterion_set, optimizer, epochs, scheduler, device = 'cpu', board_writer=None, hook=False,  save_folder = None):\n",
        "    model = model.to(device)\n",
        "    valid_loss_min = np.Inf\n",
        "    start = time.time()\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(f'Epoch {epoch}/{epochs-1}')\n",
        "        print('-'*15)\n",
        "\n",
        "        for phase in ['train','val']:\n",
        "            \n",
        "            running_loss = 0.0\n",
        "            running_correct = 0\n",
        "\n",
        "            class_probs = []\n",
        "            class_preds = []\n",
        "            y_target = np.array([])\n",
        "\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "                loader = train_loader\n",
        "                dataset_size = dataset_sizes[0]\n",
        "                filename = save_folder+'/train.csv'\n",
        "                criterion = criterion_set[0]\n",
        "            else:\n",
        "                model.eval()\n",
        "                loader = valid_loader\n",
        "                dataset_size = dataset_sizes[1]\n",
        "                filename = save_folder+'/val.csv'\n",
        "                criterion = criterion_set[1]\n",
        "            i = 0\n",
        "            begin_phase = time.time()\n",
        "            for inputs, labels in loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                y_target = np.concatenate((y_target,labels.cpu()))\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    class_probs_batch = [F.softmax(output, dim=0) for output in outputs]\n",
        "                    class_probs.append(class_probs_batch)\n",
        "                    class_preds.append(preds.cpu())\n",
        "\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_correct += torch.sum(preds == labels.data)\n",
        "                    \n",
        "                end = time.time() - begin_phase\n",
        "                i+=1\n",
        "                if i%1000 == 0:\n",
        "                    print(f'Phase: {phase} Batch {i} complete in {end // 60:.0f}m {end % 60:.0f}s')\n",
        "                \n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
        "            epoch_preds = torch.cat(class_preds)\n",
        "\n",
        "            for i in range(len(class_names)):\n",
        "                add_pr_curve_tensorboard(board_writer, phase, i, epoch_probs, epoch_preds, global_step=epoch)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_size\n",
        "            epoch_accuracy = running_correct.double() / dataset_size\n",
        "            \n",
        "            # save model if validation loss has decreased\n",
        "            if phase == 'val' and epoch_loss <= valid_loss_min:\n",
        "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saved updated model.'.format(valid_loss_min, epoch_loss))\n",
        "                torch.save(model.state_dict(), save_folder+'/checkpoint.pt')\n",
        "                valid_loss_min = epoch_loss\n",
        "\n",
        "            log_metrics(filename, y_target, epoch_preds, epoch_loss, epoch_loss <= valid_loss_min)\n",
        "\n",
        "            time_elapsed = time.time() - start\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f}; Accuracy: {epoch_accuracy:.4f}; Completed in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "\n",
        "            if board_writer is not None:\n",
        "                board_writer.add_scalar(f'{phase}/loss', epoch_loss, epoch)\n",
        "                board_writer.add_scalar(f'{phase}/accuracy', epoch_accuracy, epoch)\n",
        "\n",
        "        print()\n",
        "\n",
        "    if board_writer is not None:\n",
        "        board_writer.flush()\n",
        "        board_writer.close()\n",
        "\n",
        "    time_elapsed = time.time() - start\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIFkeZSaCC-Q"
      },
      "source": [
        "#model settings\n",
        "model_name = 'pretrained_resnet50'\n",
        "results_folder = './results/'\n",
        "save_folder = results_folder + model_name\n",
        "train_results = save_folder +'/train.csv'\n",
        "val_results = save_folder +'/val.csv'\n",
        "checkpoint_path = save_folder +'/checkpoint.pt'\n",
        "balance_loss = True\n",
        "\n",
        "logs = SummaryWriter(f'{save_folder}/logs/{model_name}')\n",
        "\n",
        "if not os.path.isdir(results_folder):\n",
        "    os.mkdir(results_folder)\n",
        "    \n",
        "if not os.path.isdir(results_folder + model_name):\n",
        "    os.mkdir(results_folder + model_name)\n",
        "      \n",
        "with open(train_results, 'a') as train_result:\n",
        "    header = \",\".join(['loss', 'accuracy', 'tn', 'fp', 'fn', 'tp', 'precision_0', 'precision_1', 'recall_0', 'recall_1', 'f1_0', 'f1_1', 'count_0', 'count_1','auroc','\\n'])\n",
        "    train_result.write(header)\n",
        "\n",
        "with open(val_results, 'a') as val_result:\n",
        "    header = \",\".join(['loss', 'accuracy', 'tn', 'fp', 'fn', 'tp', 'precision_0', 'precision_1', 'recall_0', 'recall_1', 'f1_0', 'f1_1', 'count_0', 'count_1','auroc','saved','\\n'])\n",
        "    val_result.write(header)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5URiOuMCJdL"
      },
      "source": [
        "#visualise batch\n",
        "\n",
        "# Make a grid from batch\n",
        "inputs, classes = next(iter(valid_loader))\n",
        "img_out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(img_out, title=[class_names[x] for x in classes])\n",
        "\n",
        "#add to tensorboard\n",
        "logs.add_image('Sample input image',img_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvW6YJnyCNWm"
      },
      "source": [
        "#calculate loss weights \n",
        "tr_loss_weights = 1.0/torch.Tensor([np.sum(np.array(train_set.dataset.targets)[train_set.indices] == 0) , np.sum(np.array(train_set.dataset.targets)[train_set.indices] == 1)])\n",
        "valid_loss_weights = 1.0/torch.Tensor([np.sum(np.array(val_set.dataset.targets)[val_set.indices] == 0) , np.sum(np.array(val_set.dataset.targets)[val_set.indices] == 1)])\n",
        "test_loss_weights = 1.0/torch.Tensor([np.sum(np.array(test_set.dataset.targets)[test_set.indices] == 0) , np.sum(np.array(test_set.dataset.targets)[test_set.indices] == 1)])\n",
        "tr_loss_weights, valid_loss_weights, test_loss_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yE-QdJyCOKp"
      },
      "source": [
        "model= models.resnet50(pretrained=True)\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "#set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if balance_loss:\n",
        "    print(\"Using scaled loss function\")\n",
        "    train_criterion = nn.CrossEntropyLoss(weight=tr_loss_weights.to(device))\n",
        "    valid_criterion = nn.CrossEntropyLoss(weight=valid_loss_weights.to(device))\n",
        "else:\n",
        "    print(\"Using unscaled loss function\")\n",
        "    train_criterion = nn.CrossEntropyLoss()\n",
        "    valid_criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "criterion_set = [train_criterion, valid_criterion]\n",
        "\n",
        "'''\n",
        "# Get a batch of data\n",
        "time1 = time.time()\n",
        "images, classes = next(iter(valid_loader))\n",
        "model.to(device)\n",
        "images = images.to(device)\n",
        "out = model(images)\n",
        "time2 = time.time() - time1\n",
        "\n",
        "\n",
        "logs.add_graph(model, images)\n",
        "logs.close()\n",
        "\n",
        "print(f'{time2 // 60:.0f}m {time2 % 60:.0f}s')'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMINj42DCT3z"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etj5hZ1-CWkK"
      },
      "source": [
        "train(model, criterion_set, optimizer, epochs= 90, scheduler=exp_lr_scheduler, device = device, board_writer=logs, hook=False, save_folder = save_folder)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}