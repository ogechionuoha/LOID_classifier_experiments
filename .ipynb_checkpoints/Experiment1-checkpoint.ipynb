{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K1TwBuB0AXM-"
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.24.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15\n",
      "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "\u001b[K     |████████████████████████████████| 298 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.8/site-packages (from tensorboard) (0.34.2)\n",
      "Collecting protobuf>=3.6.0\n",
      "  Downloading protobuf-3.14.0-cp38-cp38-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (46.4.0.post20200518)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard) (1.19.2)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 3.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.35.0-cp38-cp38-manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 11.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 11.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 10.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n",
      "  Downloading rsa-4.7-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 3.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: cachetools, pyasn1, pyasn1-modules, rsa, google-auth, werkzeug, protobuf, tensorboard-plugin-wit, markdown, grpcio, absl-py, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard\n",
      "Successfully installed absl-py-0.11.0 cachetools-4.2.1 google-auth-1.24.0 google-auth-oauthlib-0.4.2 grpcio-1.35.0 markdown-3.3.3 oauthlib-3.1.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 werkzeug-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Keyq9w-AgrY"
   },
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "data = '/workspace/loid/images/old_uk_not_uk/train/'\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "tr_transform = transforms.Compose([transforms.RandomResizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "#test_transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "\n",
    "tr_dataset = datasets.ImageFolder(data, transform=tr_transform)\n",
    "#test_dataset = datasets.ImageFolder('./hymenoptera_data/val', transform=test_transform)\n",
    "\n",
    "end = time.time() - begin\n",
    "class_names = tr_dataset.classes\n",
    "class_names, f'loading complete in {end // 60:.0f}m {end % 60:.0f}s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z24_jZmqBOeX"
   },
   "outputs": [],
   "source": [
    "tr_class_counts = np.sum(np.array(tr_dataset.targets) == 0), np.sum(np.array(tr_dataset.targets) == 1)\n",
    "#test_class_counts = np.sum(np.array(test_dataset.targets) == 0), np.sum(np.array(test_dataset.targets) == 1)\n",
    "\n",
    "print(tr_class_counts)\n",
    "#, test_class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2WRgzFOBVuO"
   },
   "outputs": [],
   "source": [
    "train_count, val_count = int(0.75 * len(tr_dataset)), int(0.15 * len(tr_dataset))\n",
    "test_count = len(tr_dataset) - (train_count + val_count)\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(tr_dataset, [train_count, val_count, test_count])\n",
    "dataset_sizes = len(train_set), len(val_set), len(test_set)\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nH3Z2eGBe0X"
   },
   "outputs": [],
   "source": [
    "#loaders\n",
    "batchsize = 64\n",
    "workers = 1\n",
    "pinmemory = True\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batchsize, num_workers=workers, shuffle=True, pin_memory=pinmemory)\n",
    "valid_loader = torch.utils.data.DataLoader(val_set, batch_size=batchsize, num_workers=workers, shuffle=True, pin_memory=pinmemory)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batchsize, num_workers=workers, shuffle=True, pin_memory=pinmemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2F2tmmsVBj9E"
   },
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def add_pr_curve_tensorboard(writer, phase, targets, probs, preds, global_step=0):\n",
    "    '''\n",
    "    Takes in a target-class and plots the corresponding precision-recall curve\n",
    "    '''\n",
    "    tensorboard_preds = preds == targets\n",
    "    tensorboard_probs = probs[:, targets]\n",
    "\n",
    "    writer.add_pr_curve(f'{phase}/pr_curve/{class_names[targets]}', tensorboard_preds, tensorboard_probs, global_step=global_step)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    \n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "def log_metrics(filename, y_target, y_pred, loss, saved=False):\n",
    "    \n",
    "    with open(filename, 'a') as result:\n",
    "        a, b = confusion_matrix(y_target, y_pred)\n",
    "        tn, fp, fn, tp = *a, *b\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(y_target, y_pred)\n",
    "        precision_0, precision_1, recall_0, recall_1, f1_0, f1_1, count_0, count_1 = *precision, *recall, *f1, *support\n",
    "        accuracy = accuracy_score(y_target, y_pred)\n",
    "        auroc = roc_auc_score(y_target, y_pred)\n",
    "        line = \",\".join([str(loss), str(accuracy), str(tn), str(fp), str(fn), str(tp), str(precision_0), str(precision_1), str(recall_0), str(recall_1), str(f1_0), str(f1_1), str(count_0), str(count_1), str(auroc), str(saved)+'\\n'])\n",
    "        result.write(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0A-E7IcB-k1"
   },
   "outputs": [],
   "source": [
    "def train(model, criterion_set, optimizer, epochs, scheduler, device = 'cpu', board_writer=None, hook=False,  save_folder = None):\n",
    "    model = model.to(device)\n",
    "    valid_loss_min = np.Inf\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f'Epoch {epoch}/{epochs-1}')\n",
    "        print('-'*15)\n",
    "\n",
    "        for phase in ['train','val']:\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "\n",
    "            class_probs = []\n",
    "            class_preds = []\n",
    "            y_target = np.array([])\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                loader = train_loader\n",
    "                dataset_size = dataset_sizes[0]\n",
    "                filename = save_folder+'/train.csv'\n",
    "                criterion = criterion_set[0]\n",
    "            else:\n",
    "                model.eval()\n",
    "                loader = valid_loader\n",
    "                dataset_size = dataset_sizes[1]\n",
    "                filename = save_folder+'/val.csv'\n",
    "                criterion = criterion_set[1]\n",
    "            i = 0\n",
    "            begin_phase = time.time()\n",
    "            for inputs, labels in loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                y_target = np.concatenate((y_target,labels.cpu()))\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    class_probs_batch = [F.softmax(output, dim=0) for output in outputs]\n",
    "                    class_probs.append(class_probs_batch)\n",
    "                    class_preds.append(preds.cpu())\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_correct += torch.sum(preds == labels.data)\n",
    "                    \n",
    "                end = time.time() - begin_phase\n",
    "                i+=1\n",
    "                if i%1000 == 0:\n",
    "                    print(f'Phase: {phase} Batch {i} complete in {end // 60:.0f}m {end % 60:.0f}s')\n",
    "                \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "            epoch_preds = torch.cat(class_preds)\n",
    "\n",
    "            for i in range(len(class_names)):\n",
    "                add_pr_curve_tensorboard(board_writer, phase, i, epoch_probs, epoch_preds, global_step=epoch)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            epoch_accuracy = running_correct.double() / dataset_size\n",
    "            \n",
    "            # save model if validation loss has decreased\n",
    "            if phase == 'val' and epoch_loss <= valid_loss_min:\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saved updated model.'.format(valid_loss_min, epoch_loss))\n",
    "                torch.save(model.state_dict(), save_folder+'/checkpoint.pt')\n",
    "                valid_loss_min = epoch_loss\n",
    "\n",
    "            log_metrics(filename, y_target, epoch_preds, epoch_loss, epoch_loss <= valid_loss_min)\n",
    "\n",
    "            time_elapsed = time.time() - start\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}; Accuracy: {epoch_accuracy:.4f}; Completed in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "\n",
    "            if board_writer is not None:\n",
    "                board_writer.add_scalar(f'{phase}/loss', epoch_loss, epoch)\n",
    "                board_writer.add_scalar(f'{phase}/accuracy', epoch_accuracy, epoch)\n",
    "\n",
    "        print()\n",
    "\n",
    "    if board_writer is not None:\n",
    "        board_writer.flush()\n",
    "        board_writer.close()\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIFkeZSaCC-Q"
   },
   "outputs": [],
   "source": [
    "#model settings\n",
    "model_name = 'pretrained_resnet50'\n",
    "results_folder = './results/'\n",
    "save_folder = results_folder + model_name\n",
    "train_results = save_folder +'/train.csv'\n",
    "val_results = save_folder +'/val.csv'\n",
    "checkpoint_path = save_folder +'/checkpoint.pt'\n",
    "balance_loss = True\n",
    "\n",
    "logs = SummaryWriter(f'{save_folder}/logs/{model_name}')\n",
    "\n",
    "if not os.path.isdir(results_folder):\n",
    "    os.mkdir(results_folder)\n",
    "    \n",
    "if not os.path.isdir(results_folder + model_name):\n",
    "    os.mkdir(results_folder + model_name)\n",
    "      \n",
    "with open(train_results, 'a') as train_result:\n",
    "    header = \",\".join(['loss', 'accuracy', 'tn', 'fp', 'fn', 'tp', 'precision_0', 'precision_1', 'recall_0', 'recall_1', 'f1_0', 'f1_1', 'count_0', 'count_1','auroc','\\n'])\n",
    "    train_result.write(header)\n",
    "\n",
    "with open(val_results, 'a') as val_result:\n",
    "    header = \",\".join(['loss', 'accuracy', 'tn', 'fp', 'fn', 'tp', 'precision_0', 'precision_1', 'recall_0', 'recall_1', 'f1_0', 'f1_1', 'count_0', 'count_1','auroc','saved','\\n'])\n",
    "    val_result.write(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5URiOuMCJdL"
   },
   "outputs": [],
   "source": [
    "#visualise batch\n",
    "\n",
    "# Make a grid from batch\n",
    "inputs, classes = next(iter(valid_loader))\n",
    "img_out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(img_out, title=[class_names[x] for x in classes])\n",
    "\n",
    "#add to tensorboard\n",
    "logs.add_image('Sample input image',img_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvW6YJnyCNWm"
   },
   "outputs": [],
   "source": [
    "#calculate loss weights \n",
    "tr_loss_weights = 1.0/torch.Tensor([np.sum(np.array(train_set.dataset.targets)[train_set.indices] == 0) , np.sum(np.array(train_set.dataset.targets)[train_set.indices] == 1)])\n",
    "valid_loss_weights = 1.0/torch.Tensor([np.sum(np.array(val_set.dataset.targets)[val_set.indices] == 0) , np.sum(np.array(val_set.dataset.targets)[val_set.indices] == 1)])\n",
    "test_loss_weights = 1.0/torch.Tensor([np.sum(np.array(test_set.dataset.targets)[test_set.indices] == 0) , np.sum(np.array(test_set.dataset.targets)[test_set.indices] == 1)])\n",
    "tr_loss_weights, valid_loss_weights, test_loss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yE-QdJyCOKp"
   },
   "outputs": [],
   "source": [
    "model= models.resnet50(pretrained=True)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "#set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if balance_loss:\n",
    "    print(\"Using scaled loss function\")\n",
    "    train_criterion = nn.CrossEntropyLoss(weight=tr_loss_weights.to(device))\n",
    "    valid_criterion = nn.CrossEntropyLoss(weight=valid_loss_weights.to(device))\n",
    "else:\n",
    "    print(\"Using unscaled loss function\")\n",
    "    train_criterion = nn.CrossEntropyLoss()\n",
    "    valid_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "criterion_set = [train_criterion, valid_criterion]\n",
    "\n",
    "'''\n",
    "# Get a batch of data\n",
    "time1 = time.time()\n",
    "images, classes = next(iter(valid_loader))\n",
    "model.to(device)\n",
    "images = images.to(device)\n",
    "out = model(images)\n",
    "time2 = time.time() - time1\n",
    "\n",
    "\n",
    "logs.add_graph(model, images)\n",
    "logs.close()\n",
    "\n",
    "print(f'{time2 // 60:.0f}m {time2 % 60:.0f}s')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMINj42DCT3z"
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Etj5hZ1-CWkK"
   },
   "outputs": [],
   "source": [
    "train(model, criterion_set, optimizer, epochs= 90, scheduler=exp_lr_scheduler, device = device, board_writer=logs, hook=False, save_folder = save_folder)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Vanillaresnet50Unbalanced.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
